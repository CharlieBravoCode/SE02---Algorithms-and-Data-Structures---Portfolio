{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b22bf8",
   "metadata": {},
   "source": [
    "# Algorithms and Data Structures Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2ce721a",
   "metadata": {},
   "source": [
    "Christoph Brauer\n",
    "SE 02\n",
    "Spring Semester 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bca813",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86333f2-0e93-4195-8df2-3d3e7c713782",
   "metadata": {},
   "source": [
    "- [1. Algorithm Time Complexity and Asymptotic Notations](#AlgorithmTimeComplexityandAsymptoticNotations)\n",
    "   - [1.1. Counting Steps and RAM Model Assumptions](#CountingStepsandRAMModelAssumptions)\n",
    "      - [1.1.1. Time Complexity](#TimeComplexity)\n",
    "      - [1.1.2. Space Complexity](#SpaceComplexity)\n",
    "   - [1.2. Asymptotic Notations](#AsymptoticNotations)\n",
    "      - [1.2.1. Big O](#BigO)\n",
    "      - [1.2.2. Big Ω](#BigΩ)\n",
    "      - [1.2.3. Big Θ](#BigΘ)\n",
    "   - [1.3. Mathematical Functions](#MathematicalFunctions)\n",
    "      - [1.3.1. Common Functions in Algorithm Analysis](#CommonFunctionsinAlgorithmAnalysis)\n",
    "      - [1.3.2. Growth Rates](#GrowthRates)\n",
    "   - [1.4. Analyzing Loops](#AnalyzingLoops)\n",
    "      - [1.4.1. Nested Loops](#NestedLoops)\n",
    "      - [1.4.2. Recursive Loops](#RecursiveLoops)\n",
    "   - [1.5. Time Complexity of Searching and Sorting Algorithms](#TimeComplexityofSearchingandSortingAlgorithms)\n",
    "      - [1.5.1. Linear Search](#LinearSearch)\n",
    "      - [1.5.2. Binary Search](#BinarySearch)\n",
    "      - [1.5.3. Bubble Sort](#BubbleSort)\n",
    "      - [1.5.4. QuickSort](#QuickSort)\n",
    "- [2. Mathematical Arguing and Reasoning](#MathematicalArguingandReasoning)\n",
    "   - [2.1. Revision of Algorithm Analysis](#RevisionofAlgorithmAnalysis)\n",
    "      - [2.1.1. Time Complexity Examples](#TimeComplexityExamples)\n",
    "      - [2.1.2. Space Complexity Examples](#SpaceComplexityExamples)\n",
    "   - [2.2. Mathematical Arguing and Reasoning](#MathematicalArguingandReasoning-1)\n",
    "      - [2.2.1. Proof Techniques](#ProofTechniques)\n",
    "      - [2.2.2. Inductive Proofs](#InductiveProofs)\n",
    "   - [2.3. Logarithms Review](#LogarithmsReview)\n",
    "      - [2.3.1. Logarithm Basics and Properties](#LogarithmBasicsandProperties)\n",
    "      - [2.3.2. Logarithms in Computer Science](#LogarithmsinComputerScience)\n",
    "- [3. Computational Machine Architectures](#ComputationalMachineArchitectures)\n",
    "   - [3.1. Von Neumann Architecture](#VonNeumannArchitecture)\n",
    "      - [3.1.1. Components and Functions](#ComponentsandFunctions)\n",
    "      - [3.1.2. Memory Hierarchy](#MemoryHierarchy)\n",
    "   - [3.2. CPU Architecture](#CPUArchitecture)\n",
    "      - [3.2.1. Pipelining](#Pipelining)\n",
    "      - [3.2.2. Cache Memory](#CacheMemory)\n",
    "   - [3.3. CISC vs RISC Architectures](#CISCvsRISCArchitectures)\n",
    "      - [3.3.1. Advantages and Disadvantages](#AdvantagesandDisadvantages)\n",
    "      - [3.3.2. Evolution and Current Trends](#EvolutionandCurrentTrends)\n",
    "   - [3.4. GPU: Graphical Process Unit](#GPU)\n",
    "      - [3.4.1. GPU Architecture](#GPUArchitecture)\n",
    "      - [3.4.2. Applications in Computing](#ApplicationsinComputing)\n",
    "   - [3.5. Building Parallel Algorithms](#BuildingParallelAlgorithms)\n",
    "      - [3.5.1. Parallelism Techniques](#ParallelismTechniques)\n",
    "      - [3.5.2. Multithreading and Concurrency](#MultithreadingandConcurrency)\n",
    "- [4. Graph Data Structure and Search Algorithms](#GraphDataStructureandSearchAlgorithms)\n",
    "   - [4.1. Graph Data Structure Overview](#GraphDataStructureOverview)\n",
    "      - [4.1.1. Terminology and Representation](#TerminologyandRepresentation)\n",
    "      - [4.1.2. Types of Graphs](#TypesofGraphs)\n",
    "   - [4.2. Graph Applications in Real Life](#GraphApplicationsinRealLife)\n",
    "      - [4.2.1. Social Networks](#SocialNetworks)\n",
    "      - [4.2.2. Navigation Systems](#NavigationSystems)\n",
    "   - [4.3. Search Algorithms on Graph Data Structure](#SearchAlgorithmsonGraphDataStructure)\n",
    "      - [4.3.1. Breadth-first Search (BFS)](#BreadthfirstSearch)\n",
    "      - [4.3.2. Depth-first Search (DFS)](#DepthfirstSearch)\n",
    "      - [4.3.3. Dijkstra's Algorithm](#DijkstrasAlgorithm)\n",
    "      - [4.3.4. A* Algorithm](#AStarAlgorithm)\n",
    "\n",
    "- [5. Sources](#Sources)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98a5c7fd-6e9a-47ca-aa86-7fc16614c5ca",
   "metadata": {},
   "source": [
    "# Algorithm Time Complexity and Asymptotic Notations <a name=\"AlgorithmTimeComplexityandAsymptoticNotations\"></a>\n",
    "\n",
    "Algorithm time complexity and asymptotic notations are fundamental concepts in computer science and algorithm analysis. Time complexity is a measure of the amount of time an algorithm takes to complete its tasks as a function of the input size. Asymptotic notations, on the other hand, provide a way to describe the growth of functions and compare their efficiency. Understanding these concepts allows developers to design, implement, and optimize algorithms that can efficiently solve computational problems. By studying algorithm time complexity and asymptotic notations, one can gain insights into how well an algorithm will perform under various circumstances and make informed decisions when choosing or designing algorithms for different applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a58d584a-4499-4136-ab3f-1fffc8b7c049",
   "metadata": {},
   "source": [
    "## 1.1. Counting Steps and RAM Model Assumptions <a name=\"CountingStepsandRAMModelAssumptions\"></a>\n",
    "In order to analyze an algorithm's time complexity, we need to count the number of elementary operations or steps that the algorithm performs. Counting steps allows us to quantify the resources an algorithm consumes, which can help us compare its efficiency with that of other algorithms. However, counting steps accurately can be challenging, as the number of steps may vary depending on the hardware and software environment.\n",
    "\n",
    "To simplify this process, we often use the Random Access Machine (RAM) model of computation, which makes several assumptions to provide a consistent and easily understandable framework for algorithm analysis. The RAM model assumes that:\n",
    "\n",
    "- Each elementary operation takes a constant amount of time.\n",
    "- Memory access times are constant and uniform, regardless of the memory location.\n",
    "- Instructions can be executed sequentially, one after another.\n",
    "\n",
    "By making these assumptions, the RAM model allows us to focus on the key aspects of an algorithm's performance, abstracting away the hardware and software details that can make counting steps more complex. With the RAM model, we can concentrate on the high-level structure of the algorithm and count the number of elementary operations as a function of input size. This function forms the basis for determining the algorithm's time complexity, which will be discussed in the following sections.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a863f2f-b948-4000-afcb-9083ca69d6c5",
   "metadata": {},
   "source": [
    "### 1.1.1. Time Complexity <a name=\"TimeComplexity\"></a>\n",
    "Time complexity is a measure of the efficiency of an algorithm in terms of the time it takes to execute as a function of the input size. It reflects the number of elementary operations or steps an algorithm performs to complete its tasks, and provides a means to compare the performance of different algorithms. Understanding the time complexity of an algorithm is crucial, as it helps developers optimize their code and choose the most efficient algorithm for a given problem.\n",
    "\n",
    "When analyzing the time complexity of an algorithm, we focus on its growth rate rather than the absolute number of steps. This is because the growth rate indicates how the algorithm's performance scales as the input size increases, which is more important for determining its efficiency. We express the time complexity of an algorithm using asymptotic notations, which will be covered in the next section.\n",
    "\n",
    "There are three main types of time complexity that I typically consider:\n",
    "\n",
    "1. Best-case time complexity: The minimum number of steps an algorithm takes to solve a problem, given the most favorable input. While this provides a loIr bound on the algorithm's performance, it is not always a useful indicator, as real-world inputs might not be optimal.\n",
    "\n",
    "2. Worst-case time complexity: The maximum number of steps an algorithm takes to solve a problem, given the least favorable input. This provides an upper bound on the algorithm's performance, and is often the most useful metric for comparing algorithms, as it guarantees a certain level of efficiency regardless of the input.\n",
    "\n",
    "3. Average-case time complexity: The average number of steps an algorithm takes to solve a problem, considering all possible inputs. This provides a more realistic estimate of the algorithm's performance in practice but can be more challenging to compute.\n",
    "\n",
    "In algorithm analysis, we usually focus on the worst-case time complexity, as it ensures a consistent level of performance and allows for meaningful comparisons between different algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33274c10-85d7-4240-adaa-463a1a2c5958",
   "metadata": {},
   "source": [
    "### 1.1.2. Space Complexity <a name=\"SpaceComplexity\"></a>\n",
    "Space complexity is another crucial aspect of algorithm efficiency that measures the amount of memory an algorithm consumes as a function of the input size. It takes into account the storage required for the input data, temporary variables, and auxiliary data structures used by the algorithm. Similar to time complexity, understanding the space complexity of an algorithm is essential for optimizing code and selecting the most efficient algorithm for a given problem, especially when memory resources are limited.\n",
    "\n",
    "When analyzing the space complexity of an algorithm, we focus on its growth rate in relation to the input size. This allows us to determine how the algorithm's memory consumption scales as the input size increases, which is crucial for assessing its efficiency in various scenarios.\n",
    "\n",
    "There are two main components of space complexity:\n",
    "\n",
    "1. Fixed space: This refers to the memory required for the input data, constants, and simple variables that do not depend on the input size. Fixed space usually does not contribute significantly to the overall space complexity, as it remains constant regardless of the input size.\n",
    "\n",
    "2. Variable space: This refers to the memory required for data structures, such as arrays or linked lists, that depend on the input size. Variable space is the primary factor contributing to the space complexity of an algorithm, as it directly correlates with the input size.\n",
    "\n",
    "When calculating the space complexity of an algorithm, we typically focus on the variable space, as it represents the memory consumption that grows with the input size. Similar to time complexity, we express the space complexity of an algorithm using asymptotic notations, which provide a concise and standardized way to describe the growth of functions and compare the efficiency of different algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd03f2da-08ef-458c-98b1-0dff24ed1f34",
   "metadata": {},
   "source": [
    "## 1.2. Asymptotic Notations <a name=\"AsymptoticNotations\"></a>\n",
    "Asymptotic notations are mathematical tools used to describe the growth of functions, particularly the time and space complexity of algorithms. They provide a concise and standardized way to express how the performance of an algorithm changes as the input size increases, allowing for meaningful comparisons between different algorithms. Asymptotic notations focus on the growth rate of a function and ignore constant factors and lower-order terms, which makes them more suitable for evaluating the efficiency of algorithms in the context of large inputs.\n",
    "\n",
    "There are three primary asymptotic notations used in algorithm analysis: Big O (O), Big Omega (Ω) and Big Theta (Θ).\n",
    "\n",
    "These asymptotic notations allow us to analyze and compare the efficiency of different algorithms in a standardized manner, focusing on their growth rates rather than the absolute number of steps or memory consumption. By using asymptotic notations, we can gain insights into how well an algorithm will perform under various circumstances and make informed decisions when choosing or designing algorithms for different applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16328009",
   "metadata": {},
   "source": [
    "### 1.2.1. Big O <a name=\"BigO\"></a>\n",
    "Big O notation describes the upper limit of an algorithm's time or space complexity, giving us a worst-case estimate. In simpler terms, Big O notation tells us the maximum amount of time or memory an algorithm could take based on the input size. It helps us understand the highest possible growth rate of an algorithm's time or space complexity as the input size increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a615a9a0",
   "metadata": {},
   "source": [
    "### 1.2.2. Big Ω <a name=\"BigΩ\"></a>\n",
    "Big Omega notation describes the lower limit of an algorithm's time or space complexity, giving us a best-case estimate. In simpler terms, Big Omega notation tells us the minimum amount of time or memory an algorithm could take based on the input size. It helps us understand the lowest possible growth rate of an algorithm's time or space complexity as the input size increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53ce2644",
   "metadata": {},
   "source": [
    "### 1.2.3. Big Θ <a name=\"BigΘ\"></a>\n",
    "Big Theta notation describes the exact growth rate of an algorithm's time or space complexity, giving us an average-case estimate. In simpler terms, Big Theta notation tells us the typical amount of time or memory an algorithm would take based on the input size. It helps us understand the actual growth rate of an algorithm's time or space complexity as the input size increases, when both the upper and lower limits are the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45174a80",
   "metadata": {},
   "source": [
    "## 1.3. Mathematical Functions <a name=\"MathematicalFunctions\"></a>\n",
    "Mathematical functions play a significant role in algorithm analysis, as they are used to represent the time and space complexity of algorithms. In this context, a function maps the input size of a problem (usually denoted as n) to the number of elementary operations or memory units required by the algorithm. By examining various mathematical functions and their properties, we can better understand the efficiency of different algorithms and how their performance scales with the input size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6c76a31",
   "metadata": {},
   "source": [
    "### 1.3.1. Common Functions in Algorithm Analysis <a name=\"CommonFunctionsinAlgorithmAnalysis\"></a>\n",
    "There are several common mathematical functions frequently encountered in algorithm analysis:\n",
    "\n",
    "1. Constant (O(1)): A constant function represents an algorithm whose time or space complexity does not depend on the input size. The performance of such an algorithm remains constant regardless of the input size.\n",
    "\n",
    "2. Linear (O(n)): A linear function represents an algorithm whose time or space complexity increases proportionally with the input size. The performance of such an algorithm degrades linearly as the input size grows.\n",
    "\n",
    "3. Quadratic (O(n²)): A quadratic function represents an algorithm whose time or space complexity increases with the square of the input size. The performance of such an algorithm degrades rapidly as the input size grows, making it inefficient for large inputs.\n",
    "\n",
    "4. Cubic (O(n³)): A cubic function represents an algorithm whose time or space complexity increases with the cube of the input size. Similar to quadratic functions, the performance of such an algorithm degrades rapidly with increasing input size, making it inefficient for large inputs.\n",
    "\n",
    "5. Logarithmic (O(log n)): A logarithmic function represents an algorithm whose time or space complexity increases logarithmically with the input size. The performance of such an algorithm degrades slowly as the input size grows, making it efficient for large inputs.\n",
    "\n",
    "6. Exponential (O(2^n)): An exponential function represents an algorithm whose time or space complexity increases exponentially with the input size. The performance of such an algorithm degrades extremely rapidly as the input size grows, making it impractical for all but the smallest inputs.\n",
    "\n",
    "7. Factorial (O(n!)): A factorial function represents an algorithm whose time or space complexity increases with the factorial of the input size. The performance of such an algorithm degrades extremely rapidly, making it impractical for all but the smallest inputs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afd0b617",
   "metadata": {},
   "source": [
    "### 1.3.2. Growth Rates <a name=\"GrowthRates\"></a>\n",
    "Understanding the growth rates of these mathematical functions is essential for comparing the efficiency of different algorithms. The order of growth of a function determines how quickly its value increases as the input size grows. Functions with lower growth rates are generally more efficient than those with higher growth rates, especially for large input sizes.\n",
    "\n",
    "Here is a ranking of the common functions mentioned above in terms of their growth rates, from the slowest to the fastest:\n",
    "\n",
    "1. Constant (O(1))\n",
    "2. Logarithmic (O(log n))\n",
    "3. Linear (O(n))\n",
    "4. Linearithmic (O(n log n))\n",
    "5. Quadratic (O(n²))\n",
    "6. Cubic (O(n³))\n",
    "7. Exponential (O(2^n))\n",
    "8. Factorial (O(n!))\n",
    "\n",
    "By understanding the growth rates of these functions, we can make informed decisions when choosing or designing algorithms for different applications, aiming for the most efficient algorithm possible given the problem's constraints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5159540",
   "metadata": {},
   "source": [
    "## 1.4. Analyzing Loops <a name=\"AnalyzingLoops\"></a>\n",
    "Analyzing loops is a crucial aspect of algorithm analysis, as they often dictate the overall time complexity of an algorithm. Loops are used to perform repetitive tasks and can greatly influence the efficiency of an algorithm. Understanding the time complexity of loops allows us to optimize our algorithms and make informed decisions when choosing or designing algorithms for different applications.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc74135",
   "metadata": {},
   "source": [
    "### 1.4.1. Nested Loops <a name=\"NestedLoops\"></a>\n",
    "Nested loops occur when a loop is placed inside another loop. The time complexity of nested loops is generally the product of the time complexities of the individual loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e23b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_loops_example(n, m):\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "n = 4\n",
    "m = 3\n",
    "result = nested_loops_example(n, m)\n",
    "print(f\"The operation inside the nested loops was executed {result} times.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39811fd7",
   "metadata": {},
   "source": [
    "In this example, the outer loop runs n times, and the inner loop runs m times. The operation inside the inner loop will be executed n * m times, resulting in a time complexity of O(nm). If both n and m are the same, the time complexity would be O(n²)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f069083",
   "metadata": {},
   "source": [
    "### 1.4.2. Recursive Loops <a name=\"RecursiveLoops\"></a>\n",
    "Recursive loops occur when a function calls itself to perform a task repeatedly. The time complexity of recursive loops depends on the number of recursive calls made and the complexity of the operations performed inside the recursive function.\n",
    "\n",
    "Here's a simple example of a recursive function in Python implementing the Fibonacci sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "\n",
    "n = 10\n",
    "result = fibonacci(n)\n",
    "print(f\"The {n}-th number in the Fibonacci sequence is {result}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed55cf5c",
   "metadata": {},
   "source": [
    "In this example, the fibonacci function calls itself twice in each step (except for the base cases when n is 0 or 1). The time complexity of this algorithm is O(2^n), which makes it highly inefficient for large input sizes.\n",
    "\n",
    "Analyzing loops, whether they are nested or recursive, is essential for understanding and optimizing the performance of algorithms. By identifying the time complexity of the loops in our code, we can make better decisions when designing and implementing algorithms and ultimately create more efficient solutions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4156f807",
   "metadata": {},
   "source": [
    "## 1.5. Time Complexity of Searching and Sorting Algorithms <a name=\"TimeComplexityofSearchingandSortingAlgorithms\"></a>\n",
    "Searching and sorting algorithms are fundamental operations in computer science, and their efficiency can greatly impact the performance of various applications. Understanding the time complexity of these algorithms allows us to choose the best algorithm for a specific task and optimize our code accordingly. In this section, we will discuss the time complexity of some common searching and sorting algorithms and provide Python code examples that can be run inside a Jupyter notebook.\n",
    "\n",
    "These examples of searching and sorting algorithms illustrate the importance of understanding the time complexity of various algorithms. By knowing the time complexity, we can make better decisions when selecting the most suitable algorithm for a specific task or optimizing our code.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ff25ac4",
   "metadata": {},
   "source": [
    "### 1.5.1. Linear Search <a name=\"LinearSearch\"></a>\n",
    "Linear search is a simple searching algorithm that iterates through each element in a list until the target element is found or the list is exhausted. The time complexity of linear search is O(n), where n is the number of elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e599ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search(arr, target):\n",
    "    for i, element in enumerate(arr):\n",
    "        if element == target:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "arr = [1, 5, 8, 12, 16, 20, 25]\n",
    "target = 12\n",
    "result = linear_search(arr, target)\n",
    "print(f\"Linear search found the target {target} at index {result}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9182697c",
   "metadata": {},
   "source": [
    "### 1.5.3. Bubble Sort <a name=\"BubbleSort\"></a>\n",
    "Bubble sort is a simple sorting algorithm that repeatedly steps through the list, comparing adjacent elements and swapping them if they are in the wrong order. The algorithm continues until no more swaps are needed. The time complexity of bubble sort is O(n^2), where n is the number of elements in the list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73adc6a",
   "metadata": {},
   "source": [
    "### 1.5.2. Binary Search <a name=\"BinarySearch\"></a>\n",
    "Binary search is an efficient searching algorithm that works on sorted lists. It repeatedly divides the list in half, comparing the middle element with the target value. If the middle element is equal to the target, the search is successful. If the middle element is greater than the target, the search continues on the left half of the list; if it's less than the target, it continues on the right half. The time complexity of binary search is O(log n), where n is the number of elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "\n",
    "    return -1\n",
    "\n",
    "arr = [1, 5, 8, 12, 16, 20, 25]\n",
    "target = 12\n",
    "result = binary_search(arr, target)\n",
    "print(f\"Binary search found the target {target} at index {result}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47b4818d",
   "metadata": {},
   "source": [
    "### 1.5.4. QuickSort <a name=\"QuickSort\"></a>\n",
    "Quicksort is an efficient sorting algorithm that works by selecting a 'pivot' element from the list and partitioning the other elements into two groups: those less than the pivot and those greater than the pivot. The algorithm then recursively sorts the two groups. The average time complexity of quicksort is O(n log n), where n is the number of elements in the list. However, the worst-case time complexity is O(n^2), which occurs when the pivot selection consistently results in an unbalanced partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8257a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(arr, low, high):\n",
    "    pivot = arr[high]\n",
    "    i = low - 1\n",
    "\n",
    "    for j in range(low, high):\n",
    "        if arr[j] <= pivot:\n",
    "            i += 1\n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "\n",
    "    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
    "    return i + 1\n",
    "\n",
    "def quick_sort(arr, low, high):\n",
    "    if low < high:\n",
    "        pivot_index = partition(arr, low, high)\n",
    "        quick_sort(arr, low, pivot_index - 1)\n",
    "        quick_sort(arr, pivot_index + 1, high)\n",
    "    return arr\n",
    "\n",
    "arr = [64, 34, 25, 12, 22, 11, 90]\n",
    "result = quick_sort(arr, 0, len(arr) - 1)\n",
    "print(f\"Quicksort sorted the list as: {result}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c67f22",
   "metadata": {},
   "source": [
    "# 2. Mathematical Arguing and Reasoning <a name=\"MathematicalArguingandReasoning\"></a>\n",
    "Mathematical arguing and reasoning are essential skills in computer science and algorithm analysis, as they allow us to rigorously establish the correctness and efficiency of algorithms. In this section, we will explore some common proof techniques and mathematical concepts used in the analysis of algorithms. We will use plain language and simple math examples to ensure a clear understanding of these concepts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b13fbbf",
   "metadata": {},
   "source": [
    "## 2.1. Revision of Algorithm Analysis <a name=\"RevisionofAlgorithmAnalysis\"></a>\n",
    "Before diving into mathematical arguing and reasoning, let's briefly review the key concepts in algorithm analysis that we discussed earlier. Time complexity is a measure of the number of operations an algorithm takes to complete its tasks as a function of the input size, while space complexity measures the memory consumed by an algorithm. Asymptotic notations, such as Big O, Big Ω, and Big Θ, help us describe the growth of functions and compare the efficiency of different algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bef08f96",
   "metadata": {},
   "source": [
    "### 2.1.1. Time Complexity Examples <a name=\"TimeComplexityExamples\"></a>\n",
    "In this section, we will look at a few simple examples of time complexity to reinforce our understanding of the concept. Recall that time complexity is a measure of the number of operations an algorithm takes to complete its tasks as a function of the input size.\n",
    "\n",
    "Example 1: Sum of an array\n",
    "\n",
    "Suppose we have an array of integers, and we want to calculate the sum of its elements. We can write a simple algorithm that iterates through the array and adds each element to a running total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7587edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_sum(arr):\n",
    "    total = 0\n",
    "    for num in arr:\n",
    "        total += num\n",
    "    return total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54575cf5",
   "metadata": {},
   "source": [
    "For an array of length n, the algorithm performs a single addition operation for each element, resulting in a total of n operations. Therefore, the time complexity of this algorithm is O(n).\n",
    "\n",
    "Example 2: Matrix multiplication\n",
    "\n",
    "Consider the problem of multiplying two square matrices of size n x n. A naive algorithm for matrix multiplication involves three nested loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    n = len(A)\n",
    "    result = [[0] * n for _ in range(n)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(n):\n",
    "                result[i][j] += A[i][k] * B[k][j]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5add3142",
   "metadata": {},
   "source": [
    "In this algorithm, each loop iterates n times, and there is a constant number of operations inside the innermost loop. Therefore, the time complexity of this algorithm is O(n^3)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9bfcf29",
   "metadata": {},
   "source": [
    "### 2.1.2. Space Complexity Examples <a name=\"SpaceComplexityExamples\"></a>\n",
    "Now, let's look at some simple examples of space complexity. Space complexity measures the memory consumed by an algorithm as a function of the input size.\n",
    "\n",
    "Example 1: Reversing a string\n",
    "\n",
    "Suppose we want to reverse a string. One straightforward approach is to create a new empty string and append each character from the original string in reverse order:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41fd7c9d",
   "metadata": {},
   "source": [
    "## 2.2. Mathematical Arguing and Reasoning <a name=\"MathematicalArguingandReasoning-1\"></a>\n",
    "Mathematical arguing and reasoning involve the use of logical statements, precise definitions, and mathematical techniques to establish the validity of various claims, such as the correctness and efficiency of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_string(s):\n",
    "    reversed_s = ''\n",
    "    for i in range(len(s) - 1, -1, -1):\n",
    "        reversed_s += s[i]\n",
    "    return reversed_s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d848284f",
   "metadata": {},
   "source": [
    "For an input string of length n, this algorithm creates a new string of length n. Therefore, the space complexity of this algorithm is O(n), as it needs to store the reversed string in memory.\n",
    "\n",
    "Example 2: Finding the maximum element in an array\n",
    "\n",
    "Consider the problem of finding the maximum element in an array of integers. We can iterate through the array and maintain a variable to store the maximum value found so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae48a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(arr):\n",
    "    max_value = arr[0]\n",
    "    for num in arr:\n",
    "        if num > max_value:\n",
    "            max_value = num\n",
    "    return max_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a17f1003",
   "metadata": {},
   "source": [
    "In this algorithm, regardless of the input size n, we only need to store a single variable to keep track of the maximum value. The space complexity of this algorithm is O(1), as the memory usage does not depend on the input size.\n",
    "\n",
    "By examining these examples, you can better understand how time complexity and space complexity are used to analyze the efficiency of algorithms, enabling you to make informed decisions when selecting or designing algorithms for different applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0bc2ae9",
   "metadata": {},
   "source": [
    "### 2.2.1. Proof Techniques <a name=\"ProofTechniques\"></a>\n",
    "There are several common proof techniques used in computer science:\n",
    "\n",
    "1. Direct proof: In a direct proof, we demonstrate the truth of a statement by a series of logical deductions based on known facts or previously proven statements. For example, to prove that the sum of two even numbers is always even, we can directly show that the sum can be expressed as 2 * (a + b), where a and b are integers.\n",
    "\n",
    "2. Proof by contradiction: In a proof by contradiction, we assume the opposite of the statement we want to prove and then show that this assumption leads to a contradiction. For example, to prove that there is no largest prime number, we can assume that there is a largest prime number, multiply all the prime numbers up to that point and add 1, which results in a number that is not divisible by any of the primes, thus contradicting our initial assumption.\n",
    "\n",
    "3. Proof by induction: Proof by induction is a technique used to prove statements involving a positive integer variable. It consists of two steps: the base case and the inductive step. In the base case, we prove that the statement is true for a specific value, usually the smallest value of the variable. In the inductive step, we assume that the statement is true for a certain value and then prove that it is also true for the next value. For example, to prove that the sum of the first n natural numbers is equal to n * (n + 1) / 2, we can first show that it is true for n = 1 (base case) and then prove that if it is true for n = k, it is also true for n = k + 1 (inductive step)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18427c56",
   "metadata": {},
   "source": [
    "### 2.2.2. Inductive Proofs <a name=\"InductiveProofs\"></a>\n",
    "Inductive proofs are particularly useful in the analysis of algorithms, as they often involve proving properties of algorithms that hold for all input sizes. Let's take a look at a simple example using the concept of mathematical induction.\n",
    "\n",
    "Example: Prove that the sum of the first n odd numbers is equal to n^2.\n",
    "\n",
    "- Base case (n = 1): The sum of the first odd number (1) is 1, which is equal to 1^2. The statement is true for n = 1.\n",
    "- Inductive step: Assume that the statement is true for n = k, i.e., the sum of the first k odd numbers is equal to k^2. We need to prove that the statement is also true for n = k + 1.\n",
    "        The sum of the first k + 1 odd numbers can be written as:\n",
    "            (1 + 3 + 5 + ... + (2k - 1)) + (2k + 1)\n",
    "            By our induction assumption, the sum of the first k odd numbers is k^2, so the expression becomes:\n",
    "            k^2 + (2k + 1)\n",
    "        Now, let's simplify the expression:\n",
    "            k^2 + (2k + 1) = k^2 + 2k + 1 = (k + 1)^2\n",
    "        This shows that the sum of the first k + 1 odd numbers is equal to (k + 1)^2. Thus, by the principle of mathematical induction, the statement is true for all positive integers n."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a52edaa",
   "metadata": {},
   "source": [
    "## 2.3. Logarithms Review <a name=\"LogarithmsReview\"></a>\n",
    "Logarithms are an essential concept in computer science and the analysis of algorithms. They are particularly useful for describing the growth rates of functions, especially in the context of time complexity. In this section, we will provide a brief review of logarithms and their properties, followed by an example of how logarithms are applied in computer science."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e6a1d05",
   "metadata": {},
   "source": [
    "### 2.3.1. Logarithm Basics and Properties <a name=\"LogarithmBasicsandProperties\"></a>\n",
    "A logarithm is the inverse operation of exponentiation. In other words, logarithms tell us the power to which a given base must be raised to produce a specific number. The logarithm of a number x with base b is denoted as log_b(x) and can be defined by the equation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1c6dd08",
   "metadata": {},
   "source": [
    "b^(log_b(x)) = x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56516026",
   "metadata": {},
   "source": [
    "For example, log_2(8) = 3 because 2^3 = 8. Here, the base is 2, and the logarithm tells us that we need to raise 2 to the power of 3 to obtain the number 8.\n",
    "\n",
    "Some essential properties of logarithms are:\n",
    "\n",
    "1. log_b(1) = 0, because any number raised to the power of 0 is 1.\n",
    "2. log_b(x * y) = log_b(x) + log_b(y), which shows that logarithms convert multiplication to addition.\n",
    "3. log_b(x / y) = log_b(x) - log_b(y), which shows that logarithms convert division to subtraction.\n",
    "4. log_b(x^p) = p * log_b(x), which shows that logarithms can move exponents to coefficients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90b05b0a",
   "metadata": {},
   "source": [
    "### 2.3.2. Logarithms in Computer Science <a name=\"LogarithmsinComputerScience\"></a>\n",
    "Logarithms frequently appear in the analysis of algorithms, especially in cases where a problem is repeatedly divided into smaller subproblems. One common example is the binary search algorithm.\n",
    "\n",
    "Binary search is an efficient algorithm for finding a specific value in a sorted list. The algorithm works by repeatedly dividing the list in half until the desired value is found or it's determined that the value is not in the list. At each step, the algorithm eliminates half of the remaining elements, so the number of elements to search is reduced by a factor of 2.\n",
    "\n",
    "Here's a simple Python implementation of binary search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(arr, target):\n",
    "    low, high = 0, len(arr) - 1\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_val = arr[mid]\n",
    "\n",
    "        if mid_val == target:\n",
    "            return mid\n",
    "        elif mid_val < target:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "436c3096",
   "metadata": {},
   "source": [
    "For a list of length n, the algorithm takes at most log_2(n) steps to find the target value or determine that it's not in the list. Therefore, the time complexity of binary search is O(log n).\n",
    "\n",
    "Understanding logarithms and their properties is crucial in computer science, as it helps to analyze and optimize algorithms that involve repeated division of problems or data structures. Mastering logarithms will enable you to design more efficient solutions to complex problems and make informed decisions when selecting algorithms for various applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33487969",
   "metadata": {},
   "source": [
    "# 3. Computational Machine Architectures <a name=\"ComputationalMachineArchitectures\"></a>\n",
    "Computational machine architectures define how computers are organized and how they execute instructions. Understanding these architectures helps programmers design efficient algorithms and optimize software performance. In this section, we will discuss the Von Neumann architecture, CPU architecture, CISC vs. RISC architectures, and the role of the Graphics Processing Unit (GPU) in modern computing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5840cb76",
   "metadata": {},
   "source": [
    "## 3.1. Von Neumann Architecture <a name=\"VonNeumannArchitecture\"></a>\n",
    "The Von Neumann architecture is a widely used model for computer design, named after mathematician and computer scientist John von Neumann. This architecture consists of several key components:\n",
    "\n",
    "1. Memory: Stores both instructions and data.\n",
    "2. Central Processing Unit (CPU): Executes instructions and processes data.\n",
    "3. Input/Output (I/O) devices: Allow communication between the computer and external devices, such as keyboards, mice, and monitors.\n",
    "\n",
    "The Von Neumann architecture operates on the stored-program concept, which means that programs and data are stored together in memory. This allows the CPU to fetch, decode, and execute instructions one after the other in a sequential manner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "497e7cfb",
   "metadata": {},
   "source": [
    "### 3.1.1. Components and Functions <a name=\"ComponentsandFunctions\"></a>\n",
    "Memory: \n",
    "The memory component of the Von Neumann architecture stores both instructions and data. It is typically divided into an addressable array of storage cells, each of which can hold a fixed amount of information (usually one byte).\n",
    "\n",
    "Central Processing Unit (CPU): \n",
    "The CPU is responsible for executing instructions and processing data. It contains the Arithmetic Logic Unit (ALU), which performs arithmetic and logical operations, and the Control Unit (CU), which manages the flow of data between the memory, ALU, and I/O devices.\n",
    "\n",
    "Input/Output (I/O) devices: \n",
    "These devices enable communication between the computer and external components. Input devices, such as keyboards and mice, allow users to enter data, while output devices, like monitors and printers, display or produce the results of computations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9451d56a",
   "metadata": {},
   "source": [
    "### 3.1.2. Memory Hierarchy <a name=\"MemoryHierarchy\"></a>\n",
    "The memory hierarchy in the Von Neumann architecture is organized into multiple layers, each with different performance characteristics and capacities. The hierarchy typically includes:\n",
    "\n",
    "1. Registers: Small, fast storage locations within the CPU.\n",
    "2. Cache memory: Faster, more expensive memory located closer to the CPU. It is used to store frequently accessed data and instructions.\n",
    "3. Main memory (RAM): Stores program instructions and data while the computer is running.\n",
    "4. Secondary storage: Non-volatile storage devices, such as hard drives and solid-state drives, used for long-term data storage.\n",
    "\n",
    "The memory hierarchy aims to provide an optimal balance between speed and storage capacity, allowing for efficient data access and processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "514ac80a",
   "metadata": {},
   "source": [
    "## 3.2. CPU Architecture <a name=\"CPUArchitecture\"></a>\n",
    "The CPU is the heart of the computer, responsible for processing instructions and data. Modern CPUs are designed with several features that enhance their performance, such as pipelining and cache memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62c6e913",
   "metadata": {},
   "source": [
    "### 3.2.1. Pipelining <a name=\"Pipelining\"></a>\n",
    "Pipelining is a technique that improves the CPU's throughput by allowing it to process multiple instructions simultaneously. In a pipelined CPU, the execution of instructions is divided into several stages, with each stage handling a different part of the instruction. This allows the CPU to work on several instructions at the same time, increasing the overall speed of execution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72e99f75",
   "metadata": {},
   "source": [
    "### 3.2.2. Cache Memory <a name=\"CacheMemory\"></a>\n",
    "Cache memory is a small, fast memory located close to the CPU. It is used to store frequently accessed data and instructions, reducing the time needed for the CPU to access them. Cache memory can be organized into different levels (L1, L2, and L3) based on their proximity to the CPU and their capacity. L1 cache is the smallest and fastest, while L3 cache is larger and slower but still faster than main memory (RAM).\n",
    "\n",
    "Cache memory uses various strategies to optimize its performance, such as:\n",
    "\n",
    "- Spatial locality: Data that is close together in memory is likely to be accessed together.\n",
    "- Temporal locality: Data that has been accessed recently is likely to be accessed again in the near future.\n",
    "\n",
    "These strategies help predict which data and instructions are likely to be needed next, allowing the cache to prefetch them and minimize the time the CPU spends waiting for data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "520d7550",
   "metadata": {},
   "source": [
    "## 3.3. CISC vs RISC Architectures <a name=\"CISCvsRISCArchitectures\"></a>\n",
    "CISC (Complex Instruction Set Computer) and RISC (Reduced Instruction Set Computer) are two contrasting approaches to CPU design. They differ in their instruction sets, complexity, and execution strategies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dbad0c6",
   "metadata": {},
   "source": [
    "### 3.3.1. Advantages and Disadvantages <a name=\"AdvantagesandDisadvantages\"></a>\n",
    "CISC architectures feature a large number of complex instructions that can perform multiple operations in a single instruction. This can lead to more efficient use of memory and a smaller program size. However, the complexity of CISC instructions can result in longer instruction execution times and increased power consumption.\n",
    "\n",
    "RISC architectures, on the other hand, use a smaller set of simple instructions that execute quickly. Each instruction typically performs a single operation, making them easier to optimize for performance. RISC architectures often have a higher instruction throughput, resulting in faster overall execution. However, RISC programs can be larger in size, as more instructions are needed to perform the same tasks as a CISC program."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "680dd3b9",
   "metadata": {},
   "source": [
    "### 3.3.2. Evolution and Current Trends <a name=\"EvolutionandCurrentTrends\"></a>\n",
    "Over the years, the distinction between CISC and RISC architectures has become less clear, as modern CPUs incorporate features from both designs to optimize performance. For example, many CISC processors now include RISC-like execution units to improve their instruction throughput. Similarly, RISC processors have adopted some CISC features to increase their capabilities and efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e78a13cb",
   "metadata": {},
   "source": [
    "## 3.4. GPU: Graphical Process Unit <a name=\"GPU\"></a>\n",
    "A Graphics Processing Unit (GPU) is a specialized processor designed to handle the complex calculations required for rendering graphics, particularly in 3D environments. GPUs have become increasingly important in modern computing due to their ability to perform parallel computations efficiently, making them suitable for a wide range of applications beyond graphics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55e0b83b",
   "metadata": {},
   "source": [
    "### 3.4.1. GPU Architecture <a name=\"GPUArchitecture\"></a>\n",
    "GPU architecture is designed to handle parallel processing, with many simple cores working together to perform calculations simultaneously. This makes GPUs particularly well-suited for tasks that can be divided into smaller, independent operations, such as rendering graphics or performing mathematical simulations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fa004dd",
   "metadata": {},
   "source": [
    "### 3.4.2. Applications in Computing <a name=\"ApplicationsinComputing\"></a>\n",
    "While GPUs were initially developed for graphics processing, they have since found applications in a variety of computing tasks, including:\n",
    "\n",
    "- Scientific simulations: GPUs can perform complex calculations quickly, making them ideal for simulating physical systems, weather patterns, and more.\n",
    "- Machine learning and artificial intelligence: GPUs are widely used in training neural networks and other machine learning models due to their parallel processing capabilities.\n",
    "- Cryptocurrency mining: GPUs are often used to perform the complex calculations required for mining cryptocurrencies like Bitcoin and Ethereum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4de5d122",
   "metadata": {},
   "source": [
    "## 3.5. Building Parallel Algorithms <a name=\"BuildingParallelAlgorithms\"></a>\n",
    "Parallel algorithms are designed to take advantage of multiple processing units working simultaneously to solve a problem. These algorithms can be executed on architectures such as multi-core CPUs and GPUs, which are capable of performing parallel computations. By dividing tasks into smaller, independent operations and distributing them across multiple processing units, parallel algorithms can significantly reduce computation time and improve overall performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7e0c9ea",
   "metadata": {},
   "source": [
    "### 3.5.1. Parallelism Techniques <a name=\"ParallelismTechniques\"></a>\n",
    "There are several types of parallelism used in algorithm design:\n",
    "\n",
    "- Data parallelism: Involves dividing the input data into smaller chunks and processing each chunk simultaneously. This is particularly suitable for tasks where the same operation is applied to different parts of the input data, such as image processing or matrix multiplication.\n",
    "\n",
    "- Task parallelism: Focuses on breaking down a problem into smaller, independent tasks that can be executed concurrently. This approach is useful when different tasks within a problem can be performed independently of each other, such as solving a set of unrelated equations.\n",
    "\n",
    "- Pipeline parallelism: Similar to pipelining in CPUs, this type of parallelism involves dividing a problem into stages, with each stage being executed by a different processing unit. As each stage completes, it passes its results to the next stage for further processing. This approach is useful for tasks that involve a sequence of operations, such as video encoding or speech recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "382996aa",
   "metadata": {},
   "source": [
    "### 3.5.2. Multithreading and Concurrency <a name=\"MultithreadingandConcurrency\"></a>\n",
    "When designing parallel algorithms, there are several factors to consider:\n",
    "\n",
    "- Scalability: A parallel algorithm should be able to efficiently utilize increasing numbers of processing units, resulting in improved performance as more resources are added.\n",
    "\n",
    "- Load balancing: To achieve optimal performance, work should be evenly distributed among the available processing units, avoiding situations where some units are idle while others are overloaded.\n",
    "\n",
    "- Communication and synchronization: Parallel algorithms often require communication and synchronization between processing units to share data and coordinate tasks. It's essential to minimize the overhead associated with these operations to avoid negatively impacting performance.\n",
    "\n",
    "- Granularity: The granularity of a parallel algorithm refers to the size of the tasks it assigns to processing units. Fine-grained algorithms divide tasks into many small operations, while coarse-grained algorithms use fewer, larger tasks. The choice of granularity can significantly impact the efficiency and performance of a parallel algorithm.\n",
    "\n",
    "By considering these factors and carefully designing parallel algorithms, developers can create efficient, high-performance solutions that leverage the full potential of modern computational architectures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7df7617",
   "metadata": {},
   "source": [
    "# 4. Graph Data Structure and Search Algorithms <a name=\"GraphDataStructureandSearchAlgorithms\"></a>\n",
    "Graphs are a versatile and powerful data structure used to represent relationships between objects, making them an essential tool in computer science, mathematics, and many other fields. They consist of nodes (vertices) and edges connecting these nodes, reflecting the connections between the elements in the graph. In this section, we will explore the basic concepts of graph data structures, their real-life applications, and search algorithms commonly used to traverse and analyze them.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ba0c2b",
   "metadata": {},
   "source": [
    "## 4.1. Graph Data Structure Overview <a name=\"GraphDataStructureOverview\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b4db5e",
   "metadata": {},
   "source": [
    "### 4.1.1. Terminology and Representation <a name=\"TerminologyandRepresentation\"></a>\n",
    "A graph G is defined as an ordered pair G(V, E), where V is the set of vertices and E is the set of edges connecting the vertices. Each edge can be represented as a tuple (u, v), where u and v are the vertices it connects.\n",
    "\n",
    "Graphs can be classified as directed or undirected. In undirected graphs, edges have no direction, meaning the relationship between nodes is bidirectional. In directed graphs (also called digraphs), edges have a direction, indicating a one-way relationship between nodes.\n",
    "\n",
    "There are several ways to represent graphs in computer programs, with two of the most common being adjacency lists and adjacency matrices:\n",
    "\n",
    "- Adjacency list: For each vertex, a list of its adjacent vertices is maintained. In Python, this can be implemented using dictionaries, with keys representing vertices and values being lists of adjacent vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {\n",
    "    'A': ['B', 'C'],\n",
    "    'B': ['A', 'D'],\n",
    "    'C': ['A', 'E'],\n",
    "    'D': ['B', 'E'],\n",
    "    'E': ['C', 'D']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f766664f",
   "metadata": {},
   "source": [
    "- Adjacency matrix: A two-dimensional matrix is used to represent the graph, with the rows and columns representing vertices, and the value at matrix[i][j] indicating the presence (1) or absence (0) of an edge between vertices i and j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30910886",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_matrix = [\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [1, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1],\n",
    "    [0, 0, 1, 1, 0]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b192ceb7",
   "metadata": {},
   "source": [
    "### 4.1.2. Types of Graphs <a name=\"TypesofGraphs\"></a>\n",
    "There are many types of graphs, some of which include:\n",
    "\n",
    "- Weighted graphs: Edges are assigned weights to represent attributes like distance, cost, or time. The adjacency matrix representation is commonly used for weighted graphs, with the value at matrix[i][j] representing the weight of the edge between vertices i and j.\n",
    "\n",
    "- Trees: A special type of graph where there are no cycles, and each node has at most one parent.\n",
    "\n",
    "- Bipartite graphs: Vertices can be partitioned into two disjoint sets, with every edge connecting a vertex from one set to a vertex from the other set.\n",
    "\n",
    "- Cyclic and acyclic graphs: A graph is cyclic if it contains at least one cycle, and acyclic if it contains no cycles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c348d47",
   "metadata": {},
   "source": [
    "## 4.2. Graph Applications in Real Life <a name=\"GraphApplicationsinRealLife\"></a>\n",
    "Graphs have numerous applications across various domains, some of which include:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51a08807",
   "metadata": {},
   "source": [
    "### 4.2.1. Social Networks <a name=\"SocialNetworks\"></a>\n",
    "Graphs are used to model social networks, with vertices representing people and edges representing connections or relationships between them. Analyzing these graphs can provide valuable insights into the structure and dynamics of social networks, such as identifying influential individuals or detecting communities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c48c4a46",
   "metadata": {},
   "source": [
    "### 4.2.2. Navigation Systems <a name=\"NavigationSystems\"></a>\n",
    "Graphs can be employed to model transportation networks, with vertices representing locations and edges representing roads or connections between locations. Weighted graphs, in particular, are useful here, as edge weights can represent distances, travel times, or costs. Algorithms such as Dijkstra's or A* can be applied to find the shortest or most efficient path between two points on the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53d6f90a",
   "metadata": {},
   "source": [
    "## 4.3. Search Algorithms on Graph Data Structure <a name=\"SearchAlgorithmsonGraphDataStructure\"></a>\n",
    "There are several search algorithms that can be used to traverse and analyze graph data structures. In this section, we will discuss some of the most common ones, including Breadth-first Search (BFS), Depth-first Search (DFS), Dijkstra's Algorithm, and A* Algorithm.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91e8d3c0",
   "metadata": {},
   "source": [
    "### 4.3.1. Breadth-first Search (BFS) <a name=\"BreadthfirstSearch\"></a>\n",
    "BFS is a graph traversal algorithm that explores all the vertices of a graph in breadth-first order, meaning that it visits all the neighbors of a vertex before moving on to their neighbors. BFS can be implemented using a queue data structure. Here's a simple Python implementation of BFS on an undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cac7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def bfs(graph, start):\n",
    "    visited = set()\n",
    "    queue = deque([start])\n",
    "\n",
    "    while queue:\n",
    "        vertex = queue.popleft()\n",
    "        if vertex not in visited:\n",
    "            visited.add(vertex)\n",
    "            queue.extend(neighbor for neighbor in graph[vertex] if neighbor not in visited)\n",
    "\n",
    "    return visited\n",
    "\n",
    "graph = {\n",
    "    'A': ['B', 'C'],\n",
    "    'B': ['A', 'D'],\n",
    "    'C': ['A', 'E'],\n",
    "    'D': ['B', 'E'],\n",
    "    'E': ['C', 'D']\n",
    "}\n",
    "\n",
    "print(bfs(graph, 'A'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2609ed8b",
   "metadata": {},
   "source": [
    "### 4.3.2. Depth-first Search (DFS) <a name=\"DepthfirstSearch\"></a>\n",
    "DFS is another graph traversal algorithm that explores vertices in a depth-first manner, visiting a vertex and recursively exploring its neighbors as deeply as possible before backtracking. DFS can be implemented using recursion or a stack data structure. Here's a simple Python implementation of DFS on an undirected graph using recursion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8302819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(graph, vertex, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    visited.add(vertex)\n",
    "\n",
    "    for neighbor in graph[vertex]:\n",
    "        if neighbor not in visited:\n",
    "            dfs(graph, neighbor, visited)\n",
    "\n",
    "    return visited\n",
    "\n",
    "graph = {\n",
    "    'A': ['B', 'C'],\n",
    "    'B': ['A', 'D'],\n",
    "    'C': ['A', 'E'],\n",
    "    'D': ['B', 'E'],\n",
    "    'E': ['C', 'D']\n",
    "}\n",
    "\n",
    "print(dfs(graph, 'A'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edfa7c31",
   "metadata": {},
   "source": [
    "In the following sections, we will discuss more advanced search algorithms like Dijkstra's and A* that can be used to find the shortest path between two points in a weighted graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f871012f",
   "metadata": {},
   "source": [
    "### 4.3.3. Dijkstra's Algorithm <a name=\"DijkstrasAlgorithm\"></a>\n",
    "Dijkstra's Algorithm is a popular graph search algorithm used to find the shortest path between two vertices in a weighted graph with non-negative edge weights. The algorithm maintains a set of unvisited vertices, and for each vertex, it keeps track of the shortest known distance from the starting vertex. The algorithm repeatedly selects the unvisited vertex with the smallest known distance, updates the distances of its neighbors, and marks the selected vertex as visited. Here's a simple Python implementation of Dijkstra's Algorithm on a directed graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def dijkstra(graph, start):\n",
    "    distances = {vertex: float('infinity') for vertex in graph}\n",
    "    distances[start] = 0\n",
    "    queue = [(0, start)]\n",
    "\n",
    "    while queue:\n",
    "        current_distance, current_vertex = heapq.heappop(queue)\n",
    "\n",
    "        if current_distance > distances[current_vertex]:\n",
    "            continue\n",
    "\n",
    "        for neighbor, weight in graph[current_vertex].items():\n",
    "            distance = current_distance + weight\n",
    "\n",
    "            if distance < distances[neighbor]:\n",
    "                distances[neighbor] = distance\n",
    "                heapq.heappush(queue, (distance, neighbor))\n",
    "\n",
    "    return distances\n",
    "\n",
    "graph = {\n",
    "    'A': {'B': 1, 'C': 4},\n",
    "    'B': {'A': 1, 'C': 2, 'D': 5},\n",
    "    'C': {'A': 4, 'B': 2, 'D': 1},\n",
    "    'D': {'B': 5, 'C': 1}\n",
    "}\n",
    "\n",
    "print(dijkstra(graph, 'A'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86a93fcb",
   "metadata": {},
   "source": [
    "### 4.3.4. A* Algorithm <a name=\"AStarAlgorithm\"></a>\n",
    "The A* Algorithm is a search algorithm that finds the shortest path between two vertices in a weighted graph by combining the benefits of Dijkstra's Algorithm and a heuristic function. The heuristic function estimates the cost of the shortest path from the current vertex to the goal vertex, allowing the algorithm to explore more promising paths first. A* is widely used in applications such as pathfinding in games and navigation systems. Here's a simple Python implementation of the A* Algorithm on a grid with Manhattan distance as the heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86586645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def a_star(grid, start, end):\n",
    "    def heuristic(a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def neighbors(pos):\n",
    "        for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "            x, y = pos[0] + dx, pos[1] + dy\n",
    "            if 0 <= x < len(grid) and 0 <= y < len(grid[0]) and grid[x][y] != 'X':\n",
    "                yield (x, y)\n",
    "\n",
    "    visited = set()\n",
    "    queue = [(heuristic(start, end), 0, start)]\n",
    "    costs = {start: 0}\n",
    "\n",
    "    while queue:\n",
    "        _, cost, current = heapq.heappop(queue)\n",
    "\n",
    "        if current == end:\n",
    "            return cost\n",
    "\n",
    "        if current in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(current)\n",
    "\n",
    "        for neighbor in neighbors(current):\n",
    "            new_cost = cost + 1\n",
    "            if neighbor not in costs or new_cost < costs[neighbor]:\n",
    "                costs[neighbor] = new_cost\n",
    "                heapq.heappush(queue, (new_cost + heuristic(neighbor, end), new_cost, neighbor))\n",
    "\n",
    "    return None\n",
    "\n",
    "grid = [\n",
    "    ['S', '.', '.', '.', 'X'],\n",
    "    ['.', 'X', '.', 'X', '.'],\n",
    "    ['.', '.', '.', '.', '.'],\n",
    "    ['X', 'X', '.', 'X', '.'],\n",
    "    ['.', '.', '.', 'E', 'X']\n",
    "]\n",
    "\n",
    "start = (0, 0)\n",
    "end = (4, 3)\n",
    "\n",
    "print(a_star(grid, start, end))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "357108cf",
   "metadata": {},
   "source": [
    "In this example, we have a 5x5 grid where 'S' is the start position, 'E' is the end position, and 'X' represents obstacles. The code will return the shortest path cost from the start position to the end position, avoiding obstacles, using the A* Algorithm with the Manhattan distance heuristic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f77ae82",
   "metadata": {},
   "source": [
    "# 5. Sources <a name=\"Sources\"></a>\n",
    "This notebook on \"Algorithms and Data Structures\" has been created using various sources to provide a comprehensive understanding of the concepts discussed. These sources have been instrumental in the development of the content, and we would like to acknowledge and recommend them for further reading and exploration.\n",
    "\n",
    "- Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.\n",
    "- Sedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.). Addison-Wesley Professional.\n",
    "- Skiena, S. S. (2020). The Algorithm Design Manual (3rd ed.). Springer.\n",
    "- Grossman, R., & Tuma, J. (2019). A Programmer's Guide to Computer Science: A Virtual Degree for the Self-Taught Developer (Vol. 1). CreateSpace Independent Publishing Platform.\n",
    "- Laakmann McDowell, G. (2015). Cracking the Coding Interview: 189 Programming Questions and Solutions (6th ed.). CareerCup.\n",
    "- Khan Academy. (n.d.). Algorithms. Retrieved from https://www.khanacademy.org/computing/computer-science/algorithms\n",
    "- Brilliant. (n.d.). Algorithms. Retrieved from https://brilliant.org/courses/algorithms/\n",
    "- LeetCode. (n.d.). LeetCode. Retrieved from https://leetcode.com/\n",
    "- GeeksforGeeks. (n.d.). Data Structures. Retrieved from https://www.geeksforgeeks.org/data-structures/\n",
    "- GeeksforGeeks. (n.d.). Algorithms. Retrieved from https://www.geeksforgeeks.org/fundamentals-of-algorithms/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
